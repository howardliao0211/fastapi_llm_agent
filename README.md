# ğŸš€ fastapi_llm_agent

A full-stack **LLM agent platform** built with **FastAPI** and **React**, designed for building, experimenting with, and deploying **LLM-powered agents** using a modern, production-ready architecture.

This project integrates a FastAPI backend, a React + Tailwind frontend, secure JWT authentication, database migrations, and **vLLM** for high-performance local or self-hosted LLM inference.

---

### Backend

* **FastAPI** â€“ High-performance async REST API
* **SQLAlchemy** â€“ ORM for database interaction
* **Alembic** â€“ Database migrations & schema versioning
* **JWT (JSON Web Tokens)** â€“ Authentication & authorization
* **vLLM** â€“ High-throughput LLM inference engine (OpenAI-compatible API)

### Frontend

* **React** â€“ Component-based UI framework
* **Tailwind CSS** â€“ Utility-first styling for rapid UI development

### Infrastructure & Tooling

* **Docker & Docker Compose** â€“ Containerized development & deployment
* **Pipenv** â€“ Python dependency management
* **pytest** â€“ Backend testing framework

---

## ğŸ§± Project Overview

**fastapi_llm_agent** provides:

* ğŸ”¥ A FastAPI backend for LLM-powered agent APIs
* ğŸ§  Integration with **vLLM** for local or self-hosted large models
* ğŸ” Secure authentication using JWT
* ğŸ—„ï¸ Persistent storage with SQLAlchemy + Alembic
* ğŸŒ A modern React + Tailwind chat UI
* ğŸ³ Dockerized full-stack setup for local development
* ğŸ§ª Testing infrastructure for backend services

---

## ğŸ“ Repository Structure

```
fastapi_llm_agent/
â”œâ”€â”€ backend/                # FastAPI backend & agent logic
â”‚   â””â”€â”€ alembic/            # Alembic migration files
â”‚   â”œâ”€â”€ apis/               # API routers
â”‚   â”œâ”€â”€ models/             # SQLAlchemy models
â”‚   â”œâ”€â”€ schemas/            # Pydantic schemas
â”‚   â”œâ”€â”€ core/               # Auth, config, security
â”‚   â”œâ”€â”€ db/                 # Database session & migrations
â”‚   â””â”€â”€ main.py             # FastAPI entrypoint
â”‚
â”œâ”€â”€ frontend/               # React + Tailwind frontend
â”‚
â”œâ”€â”€ docker-compose.yaml     # Multi-service stack
â”œâ”€â”€ Dockerfile              # Backend image
â”œâ”€â”€ .env.example            # Environment variables template
â”œâ”€â”€ Pipfile                 # Python dependencies
â”œâ”€â”€ Pipfile.lock
â”œâ”€â”€ pytest.ini              # Test configuration
â””â”€â”€ .github/                # CI / workflows
```

---

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/howardliao0211/fastapi_llm_agent.git
cd fastapi_llm_agent
```

---

### 2. Install Dependencies

```bash
pipenv install
pipenv shell
```

### 3. Configure Environment Variables

```bash
cp .env.example .env
```

Edit `.env` and configure:

---

## 4. Database Migrations (Alembic)

Apply migrations:

```bash
alembic upgrade head
```

Create a new migration:

```bash
alembic revision --autogenerate -m "migration message"
```

---

### 5. Run Docker

```bash
docker compose up --build
```
Open in browser:

```
http://localhost:3000
```

---

## ğŸ§  vLLM Integration

This project is designed to work with **vLLM** as an OpenAI-compatible inference server.
The backend communicates with vLLM using the OpenAI-compatible API interface.

---

## ğŸ³ Docker (Recommended)

Run the full stack using Docker Compose:

```bash
docker-compose up --build
```

This starts:

* FastAPI backend
* React frontend
* Database
* vLLM service (if configured)

---

## ğŸ§ª Testing

Run backend tests:

```bash
pipenv run pytest
```

---

## ğŸ“Œ Key Features

* âœ… JWT-based authentication
* âœ… Database-backed users, chats, and messages
* âœ… LLM agent abstraction layer
* âœ… OpenAI-compatible inference via vLLM
* âœ… Modern chat UI with React + Tailwind
* âœ… Fully dockerized workflow
* âœ… Production-ready backend architecture

---

## ğŸ›  Future Improvements

* Retrieval-Augmented Generation (RAG)
* Multi-agent orchestration
* LangChain integration
---

## ğŸ“œ License

MIT License

---

If you find this project useful, feel free to â­ the repository or open an issue with suggestions or improvements.
